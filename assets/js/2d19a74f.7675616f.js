"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[105],{62850:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var i=n(74848),s=n(28453);const o={sidebar_position:4},a="Back to the Future: Building Your Research HDB for Stock Market Insights",r={id:"tutorials/researchHDB",title:"Back to the Future: Building Your Research HDB for Stock Market Insights",description:"In our last post, we explored how to build a real-time stock market data feed, but as any good quant knows, historical data is often the crystal ball we look into when trying to forecast future trends, or at least, give it our best shot.",source:"@site/docs/tutorials/researchHDB.mdx",sourceDirName:"tutorials",slug:"/tutorials/researchHDB",permalink:"/docs/tutorials/researchHDB",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"A Real Time Stock Market Feed",permalink:"/docs/tutorials/realTimeStocks"},next:{title:"Streaming Smarter: Real-Time Market Data with Kola, Python, and Databento",permalink:"/docs/tutorials/realTimeStocksUpdate"}},d={},l=[{value:"Who is this tutorial for?",id:"who-is-this-tutorial-for",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"What\u2019s in It for You: Skills You\u2019ll Master in This Tutorial",id:"whats-in-it-for-you-skills-youll-master-in-this-tutorial",level:2},{value:"Under the Hood: The Minimalist Tech Stack Powering It All",id:"under-the-hood-the-minimalist-tech-stack-powering-it-all",level:2},{value:"The Setup",id:"the-setup",level:2},{value:"Python Data Downloader",id:"python-data-downloader",level:3},{value:"KDB/Q Data Loader &amp; Persister",id:"kdbq-data-loader--persister",level:3},{value:"Setup",id:"setup",level:4},{value:"Data Load",id:"data-load",level:4},{value:"Save Data",id:"save-data",level:4},{value:"Performance Tests",id:"performance-tests",level:4},{value:"Performance Tuning Part 1",id:"performance-tuning-part-1",level:4},{value:"Behind the Magic: The Secret Life of .Q.pdft",id:"behind-the-magic-the-secret-life-of-qpdft",level:4},{value:"Performance Tuning Part 2",id:"performance-tuning-part-2",level:4},{value:"Historical Research Database (HDB)",id:"historical-research-database-hdb",level:4},{value:"Putting it all together",id:"putting-it-all-together",level:2},{value:"Where we go from here",id:"where-we-go-from-here",level:2},{value:"Resources",id:"resources",level:2},{value:"Source code",id:"source-code",level:3}];function c(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.h1,{id:"back-to-the-future-building-your-research-hdb-for-stock-market-insights",children:"Back to the Future: Building Your Research HDB for Stock Market Insights"}),"\n",(0,i.jsxs)(t.p,{children:["In our last post, we explored how to build a ",(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/tutorials/realTimeStocks",children:"real-time stock market data feed"}),", but as any good quant knows, historical data is often the crystal ball we look into when trying to forecast future trends, or at least, give it our best shot."]}),"\n",(0,i.jsxs)(t.p,{children:["In this tutorial, you'll learn how to build your very own ",(0,i.jsx)(t.strong,{children:"historical stock market database"})," for research and analysis, completely FREE of charge. Using ",(0,i.jsx)(t.strong,{children:"Python"})," and the ",(0,i.jsx)(t.a,{href:"https://github.com/ranaroussi/yfinance",children:(0,i.jsx)(t.strong,{children:(0,i.jsx)(t.code,{children:"yfinance"})})})," library, we\u2019ll fetch historical stock data from Yahoo Finance and save it to a CSV file. From there, we\u2019ll load the data into ",(0,i.jsx)(t.strong,{children:"KDB/Q"})," and create a fully functional ",(0,i.jsx)(t.strong,{children:"Research HDB"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"Think it\u2019ll take tons of code? Think again! It\u2019s way simpler than you\u2019d expect."}),"\n",(0,i.jsx)(t.p,{children:"While this setup has been simplified for educational purposes, it can be easily adapted to resemble a more production-ready environment."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Back to the Future",src:n(26662).A+"",width:"976",height:"549"})}),"\n",(0,i.jsx)(t.h2,{id:"who-is-this-tutorial-for",children:"Who is this tutorial for?"}),"\n",(0,i.jsx)(t.p,{children:"This tutorial is for anyone keen to build a historical stock market database. Whether you're a quant or researcher looking to analyze historical market data, or a KDB/Q developer curious about the technical implementation, there's something valuable here for everyone."}),"\n",(0,i.jsx)(t.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(t.p,{children:"This tutorial is by far the easiest one I\u2019ve put together, no deep KDB/Q knowledge required, making it perfect even for q-bies (beginner KDB/Q developers) to follow along. That said, if you're curious to dive deeper into KDB/Q or explore a typical tick setup, I\u2019ve included some helpful resources at the end of the post. A basic understanding of Python can be handy, but not necessary given that our trustworthy friend Claude is back lending us a hand with the Python code."}),"\n",(0,i.jsx)(t.h2,{id:"whats-in-it-for-you-skills-youll-master-in-this-tutorial",children:"What\u2019s in It for You: Skills You\u2019ll Master in This Tutorial"}),"\n",(0,i.jsxs)(t.p,{children:["While this tutorial is relatively straightforward, it\u2019s packed with valuable insights. You\u2019ll learn how to read data from CSV files and persist it to disk using the full power of KDB/Q. We\u2019ll explore how to benchmark performance using timer functions and discuss strategies to optimise your code for speed. You\u2019ll also get an introduction to parallel processing with multi-threading and KDB/Q\u2019s built-in parallel operators. On top of that, we\u2019ll dive into the ",(0,i.jsx)(t.a,{href:"https://code.kx.com/q/ref/dotq/",children:(0,i.jsx)(t.code,{children:".Q"})})," namespace and its handy helper functions for data persistence, and then take it a step further by examining one of those functions under the hood, exploring its ",(0,i.jsx)(t.code,{children:"K"})," implementation, and building our own optimised version."]}),"\n",(0,i.jsx)(t.h2,{id:"under-the-hood-the-minimalist-tech-stack-powering-it-all",children:"Under the Hood: The Minimalist Tech Stack Powering It All"}),"\n",(0,i.jsxs)(t.p,{children:["The tech stack for this tutorial is even more minimal than in our previous one. Since we\u2019re not streaming data directly from Python to KDB/Q, we can drop the ",(0,i.jsx)(t.a,{href:"https://github.com/exxeleron/qPython/tree/master",children:(0,i.jsx)(t.code,{children:"qpython"})})," library altogether. All we need is Python with the ",(0,i.jsx)(t.a,{href:"https://github.com/ranaroussi/yfinance",children:(0,i.jsx)(t.strong,{children:(0,i.jsx)(t.code,{children:"yfinance"})})})," library to fetch historical stock data from Yahoo Finance, and, of course, our go-to language for time series and big data analysis: ",(0,i.jsx)(t.strong,{children:"KDB/Q"}),"."]}),"\n",(0,i.jsx)(t.h2,{id:"the-setup",children:"The Setup"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"KDB/Q Stock Research Historical Database",src:n(53319).A+"",width:"741",height:"511"})}),"\n",(0,i.jsx)(t.p,{children:"The architecture of our application is built around three key components:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Python Data Downloader"}),": Using the ",(0,i.jsx)(t.a,{href:"https://github.com/ranaroussi/yfinance",children:(0,i.jsx)(t.code,{children:"yfinance"})})," library, we\u2019ll fetch historical stock market data and save it as CSV files."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"KDB/Q Data Loader & Persister"}),": This component reads the CSV data into memory, processes it, and writes it to disk as part of a Historical Database (HDB)."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Historical Research Database (HDB)"}),": Finally, we configure a KDB/Q HDB to point to the persisted data on disk for analysis and querying."]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"In the following section, we\u2019ll dive into each of these components in more detail."}),"\n",(0,i.jsx)(t.h3,{id:"python-data-downloader",children:"Python Data Downloader"}),"\n",(0,i.jsx)(t.p,{children:"Just like in our previous tutorial, the Python Data Downloader is the most code-heavy component, clocking in at just over 130 lines. It reads a list of tickers from a CSV file and fetches historical stock data from Yahoo Finance based on either a specified date range and interval or a predefined period and interval. The downloaded data is then saved in a designated folder, with one CSV file per stock."}),"\n",(0,i.jsx)(t.h3,{id:"kdbq-data-loader--persister",children:"KDB/Q Data Loader & Persister"}),"\n",(0,i.jsxs)(t.p,{children:["Now let\u2019s turn our attention to the KDB/Q data loader and persister. In its simplest form, the entire script is under 20 lines of code, and we could trim it even further by eliminating some whitespace. But before we run it all at once, it\u2019s worth checking how each function performs individually. To do this, we use the ",(0,i.jsx)(t.a,{href:"https://code.kx.com/q/basics/syscmds/#ts-time-and-space",children:(0,i.jsx)(t.code,{children:"\\ts"})})," system command, which measures both time and memory usage for a given operation."]}),"\n",(0,i.jsx)(t.p,{children:"Before we dive into optimization, let\u2019s quickly review the setup we created for our data loader."}),"\n",(0,i.jsx)(t.p,{children:"First, we define an empty table schema that matches the structure of the final data we plan to load and persist. This will act as a container for all the data we process. Then, we specify the column names expected in the raw CSV files. Since we\u2019ll be enriching the data before writing it to disk, the schema of the CSV files differs from the schema of the final output. Lastly, we create a variable to store a list of all the filenames we want to process."}),"\n",(0,i.jsx)(t.p,{children:"You can find the complete source code for this project [here], but for now, we\u2019ll walk through it step by step."}),"\n",(0,i.jsx)(t.h4,{id:"setup",children:"Setup"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"data:([] date:`date$(); sym:`symbol$(); feedHandlerTime: `timestamp$(); open: `float$(); high: `float$(); low: `float$(); close: `float$(); volume: `long$(); dividends: `float$(); stockSplit: `float$());\ncolsToLoad:`date`open`high`low`close`volume`dividends`stockSplit;\nfiles:key `:.;\n"})}),"\n",(0,i.jsx)(t.h4,{id:"data-load",children:"Data Load"}),"\n",(0,i.jsxs)(t.p,{children:["Before loading any data, whether it's stock prices or something else, it's always a good idea o take a quick peek first. One easy way to do this is by using the Unix ",(0,i.jsx)(t.a,{href:"https://man7.org/linux/man-pages/man1/head.1.html",children:(0,i.jsx)(t.code,{children:"head"})})," command, which displays the first 10 lines of a file. This gives us a clear view of the structure, allowing us to understand the column headers, data types, and how we should shape our table schema in KDB/Q."]}),"\n",(0,i.jsxs)(t.p,{children:["As we inspect the content, one key detail jumps out: the dataset doesn\u2019t include the ticker symbol. A quick look at the filename reveals why, the symbol is encoded in the file name itself (",(0,i.jsx)(t.code,{children:"VOW3.DE_20250520_213458.csv"}),") and was likely omitted from the file content to avoid redundancy. That means we\u2019ll need to extract the symbol from the filename and append it to each record."]}),"\n",(0,i.jsx)(t.p,{children:"Additionally, we'll enhance our dataset further by:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Adding a ",(0,i.jsx)(t.code,{children:"feedHandlerTime"})," column to simulate the time the feed processed the data"]}),"\n",(0,i.jsxs)(t.li,{children:["Converting the ",(0,i.jsx)(t.code,{children:"date"})," column from a ",(0,i.jsx)(t.code,{children:"timestamp"})," to a ",(0,i.jsx)(t.code,{children:"date"})," type"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"These small enhancements will make our data much cleaner and more useful for analysis."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"alexanderunterrainer@Mac:~/repos/financeData/stock_data|\u21d2  head VOW3.DE_20250520_213458.csv\nDate,Open,High,Low,Close,Volume,Dividends,Stock Splits\n1998-07-22 00:00:00+02:00,21.488154257121344,22.136574506271078,21.488154257121344,21.980979919433594,56336,0.0,0.0\n1998-07-23 00:00:00+02:00,22.22316856885747,22.396011942250293,21.01258087158203,21.01258087158203,95571,0.0,0.0\n1998-07-24 00:00:00+02:00,20.75315016493062,21.185430035478102,20.37262180820672,20.925994873046875,154423,0.0,0.0\n1998-07-27 00:00:00+02:00,20.925992035048456,21.099174218262796,20.234277095880632,20.32086753845215,62875,0.0,0.0\n1998-07-28 00:00:00+02:00,20.14802370431103,20.908741550046198,19.680904936904067,19.836498260498047,116194,0.0,0.0\n1998-07-29 00:00:00+02:00,19.853748790764257,20.753147350373524,19.5425608586415,20.32086753845215,95068,0.0,0.0\n1998-07-30 00:00:00+02:00,20.40712156543796,21.012583592704,20.40712156543796,20.701396942138672,51306,0.0,0.0\n1998-07-31 00:00:00+02:00,21.012583592704,21.012583592704,19.11028206085388,20.701396942138672,145369,0.0,0.0\n1998-08-03 00:00:00+02:00,18.850849105456504,18.980397941705075,18.764257350178806,18.95469093322754,74948,0.0,0.0\n"})}),"\n",(0,i.jsx)(t.p,{children:"Our final data loading function therefore becomes:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:'loadData:{[file]\n        s:`$first "_" vs string file;\n        `data insert `date`sym`feedHandlerTime xcols update `date$date,sym:s,feedHandlerTime:date from 1_flip colsToLoad!("PFFFFJFF";csv) 0: hsym file;\n        };\n'})}),"\n",(0,i.jsx)(t.h4,{id:"save-data",children:"Save Data"}),"\n",(0,i.jsxs)(t.p,{children:["Next, we persist our data to disk using ",(0,i.jsx)(t.a,{href:"https://code.kx.com/q/ref/dotq/#dpft-save-table",children:(0,i.jsx)(t.code,{children:".Q.pdft"})}),", KDB/Q\u2019s built-in function for saving data to a partitioned Historical Database (HDB). Don\u2019t worry if you're not familiar with the different HDB types, we\u2019ll explore those in a separate post. For now, our focus is on building a research-ready database."]}),"\n",(0,i.jsxs)(t.p,{children:["To keep the code clean and easily extensible, we define a helper function to handle saving data for a single date. This approach makes it easy to optimize our code for performance later on. The function is then called by our main ",(0,i.jsx)(t.code,{children:"saveData"})," routine, which loops through all unique dates in our dataset. Best of all, this can be accomplished in just a few lines of code. Let\u2019s take a look."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:'saveOneDate:{[hdbDir;t;d]\n        `stocks set select from t where date=d;\n        .Q.dpft[hdbDir;d;`sym;`stocks];\n        delete stocks from `.;\n        };\n\nsaveData:{[]\n        d:exec distinct date from data;\n        saveOneDate[`$":/Users/alexanderunterrainer/repos/financeData/hdb/";data;] each d;\n        };\n'})}),"\n",(0,i.jsx)(t.h4,{id:"performance-tests",children:"Performance Tests"}),"\n",(0,i.jsxs)(t.p,{children:["Now, let\u2019s evaluate how our functions perform, leveraging the system command ",(0,i.jsx)(t.code,{children:"\\ts"})," mentioned earlier. This will return the execution time in milliseconds as well as how much memory was used in bytes."]}),"\n",(0,i.jsx)(t.p,{children:"First we load all our data:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)\\ts loadData each files\n53 11109488\nq)count data\n75824\nq)10#data\ndate       sym    feedHandlerTime               open     high     low      close    volume   dividends stockSplit\n-----------------------------------------------------------------------------------------------------------------\n1999.05.06 7203.T 1999.05.06D00:00:00.000000000 396.8345 410.4792 392.2863 410.4792 15575000 0         0\n1999.05.07 7203.T 1999.05.07D00:00:00.000000000 408.2051 409.3421 392.2862 395.6974 15165000 0         0\n1999.05.10 7203.T 1999.05.10D00:00:00.000000000 395.6975 400.2458 392.2863 394.5605 6305000  0         0\n1999.05.11 7203.T 1999.05.11D00:00:00.000000000 397.9717 400.2458 391.1493 391.1493 8430000  0         0\n1999.05.12 7203.T 1999.05.12D00:00:00.000000000 391.1492 397.9716 391.1492 397.9716 12980000 0         0\n1999.05.13 7203.T 1999.05.13D00:00:00.000000000 393.4233 393.4233 385.4639 388.8751 7165000  0         0\n1999.05.14 7203.T 1999.05.14D00:00:00.000000000 390.0121 390.0121 383.1897 384.3268 11475000 0         0\n1999.05.17 7203.T 1999.05.17D00:00:00.000000000 378.6415 379.7786 371.8192 372.9562 10900000 0         0\n1999.05.18 7203.T 1999.05.18D00:00:00.000000000 375.2304 387.7381 371.8192 385.464  13680000 0         0\n1999.05.19 7203.T 1999.05.19D00:00:00.000000000 374.0932 376.3673 358.1743 369.545  19680000 0         0\nq)select count i by sym from data\nsym    | x\n-------| -----\n7203.T | 6511\n9988.HK| 1345\nAAPL   | 11200\nAMZN   | 7048\nGOOG   | 5222\nMETA   | 3270\nMSFT   | 9874\nQQQ    | 6591\nSPY    | 8133\nTSLA   | 3747\nVOW3.DE| 6865\nVTI    | 6018\n"})}),"\n",(0,i.jsx)(t.p,{children:"Next, we save the data:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)\\ts saveData[]\n13312 4396288\n"})}),"\n",(0,i.jsx)(t.p,{children:"While the data loading step is impressively fast, just 53 milliseconds, the save-to-disk step takes significantly longer at 13.5 seconds, and that\u2019s only for 12 stocks. Clearly, there\u2019s room for optimization. Let\u2019s see how we can speed things up."}),"\n",(0,i.jsx)(t.h4,{id:"performance-tuning-part-1",children:"Performance Tuning Part 1"}),"\n",(0,i.jsxs)(t.p,{children:["The first optimization we\u2019ll explore is a clever technique I picked up from ",(0,i.jsx)(t.a,{href:"https://www.linkedin.com/in/nickpsaris/",children:"Nick Psaris'"})," excellent book ",(0,i.jsx)(t.a,{href:"https://amzn.to/4dAg1Hh",children:(0,i.jsx)(t.em,{children:(0,i.jsx)(t.strong,{children:"Q-Tips: Fast, Scalable and Maintainable KDB+"})})}),", a personal favorite of mine. In his book, Nick demonstrates how to accelerate an ",(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/joins#as-of-joins",children:(0,i.jsx)(t.code,{children:"as-of join"})})," by converting one of the tables into a dictionary, where each key is a date and the corresponding value is a table containing only the data for that date. Let\u2019s apply this concept to the ",(0,i.jsx)(t.code,{children:"stocks"})," data we just loaded into memory to see it in action."]}),"\n",(0,i.jsx)(t.admonition,{type:"tip",children:(0,i.jsxs)(t.p,{children:["If you need a refresher of KDB/Q joins, read my dedicated blog post ",(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/joins",children:"here"})]})}),"\n",(0,i.jsx)(t.p,{children:"First, let\u2019s take a quick glance at our dataset to understand its current structure."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)data\ndate       sym    feedHandlerTime               open     high     low      close    volume   dividends stockSplit\n-----------------------------------------------------------------------------------------------------------------\n1999.05.06 7203.T 1999.05.06D00:00:00.000000000 396.8345 410.4792 392.2863 410.4792 15575000 0         0\n1999.05.07 7203.T 1999.05.07D00:00:00.000000000 408.2051 409.3421 392.2862 395.6974 15165000 0         0\n1999.05.10 7203.T 1999.05.10D00:00:00.000000000 395.6975 400.2458 392.2863 394.5605 6305000  0         0\n1999.05.11 7203.T 1999.05.11D00:00:00.000000000 397.9717 400.2458 391.1493 391.1493 8430000  0         0\n1999.05.12 7203.T 1999.05.12D00:00:00.000000000 391.1492 397.9716 391.1492 397.9716 12980000 0         0\n1999.05.13 7203.T 1999.05.13D00:00:00.000000000 393.4233 393.4233 385.4639 388.8751 7165000  0         0\n1999.05.14 7203.T 1999.05.14D00:00:00.000000000 390.0121 390.0121 383.1897 384.3268 11475000 0         0\n1999.05.17 7203.T 1999.05.17D00:00:00.000000000 378.6415 379.7786 371.8192 372.9562 10900000 0         0\n1999.05.18 7203.T 1999.05.18D00:00:00.000000000 375.2304 387.7381 371.8192 385.464  13680000 0         0\n1999.05.19 7203.T 1999.05.19D00:00:00.000000000 374.0932 376.3673 358.1743 369.545  19680000 0         0\n1999.05.20 7203.T 1999.05.20D00:00:00.000000000 374.0932 382.0527 371.8191 377.5044 9375000  0         0\n1999.05.21 7203.T 1999.05.21D00:00:00.000000000 380.9157 383.1898 372.9562 378.6415 11610000 0         0\n1999.05.24 7203.T 1999.05.24D00:00:00.000000000 378.6415 378.6415 366.1339 378.6415 9865000  0         0\n...\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Since our goal is to organize the data by individual days, we need to identify the row indices corresponding to each unique date. This can be achieved by grouping the ",(0,i.jsx)(t.code,{children:"date"})," column in our table using the ",(0,i.jsx)(t.a,{href:"https://code.kx.com/q/ref/group/",children:(0,i.jsx)(t.code,{children:"group"})})," keyword, which gives us exactly what we need: a mapping from dates to row indices."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)group data`date\n1999.05.06| 0 12505 19553 37919 44510 52643 63147\n1999.05.07| 1 12506 19554 37920 44511 52644 63148\n1999.05.10| 2 12507 19555 37921 44512 52645 63149\n1999.05.11| 3 12508 19556 37922 44513 52646 63150\n1999.05.12| 4 12509 19557 37923 44514 52647 63151\n1999.05.13| 5 12510 19558 37924 44515 52648 63152\n1999.05.14| 6 12511 19559 37925 44516 52649 63153\n1999.05.17| 7 12512 19560 37926 44517 52650 63154\n1999.05.18| 8 12513 19561 37927 44518 52651 63155\n1999.05.19| 9 12514 19562 37928 44519 52652 63156\n1999.05.20| 10 12515 19563 37929 44520 52653 63157\n...\n"})}),"\n",(0,i.jsx)(t.p,{children:"By using this grouped dictionary to index into our original table, we build a structure where each key is a date, and each value is a sub-table containing only the data for that specific date."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)data group data`date\n1999.05.06| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.06 1999.05.06 1999.05.06 1999.05.06 1999.05.06 1999.05.06 1999.05.06;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.06D00:00:00.000000000 1999...\n1999.05.07| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.07 1999.05.07 1999.05.07 1999.05.07 1999.05.07 1999.05.07 1999.05.07;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.07D00:00:00.000000000 1999...\n1999.05.10| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.10 1999.05.10 1999.05.10 1999.05.10 1999.05.10 1999.05.10 1999.05.10;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.10D00:00:00.000000000 1999...\n1999.05.11| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.11 1999.05.11 1999.05.11 1999.05.11 1999.05.11 1999.05.11 1999.05.11;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.11D00:00:00.000000000 1999...\n1999.05.12| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.12 1999.05.12 1999.05.12 1999.05.12 1999.05.12 1999.05.12 1999.05.12;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.12D00:00:00.000000000 1999...\n1999.05.13| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.13 1999.05.13 1999.05.13 1999.05.13 1999.05.13 1999.05.13 1999.05.13;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.13D00:00:00.000000000 1999...\n1999.05.14| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.14 1999.05.14 1999.05.14 1999.05.14 1999.05.14 1999.05.14 1999.05.14;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.14D00:00:00.000000000 1999...\n1999.05.17| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.17 1999.05.17 1999.05.17 1999.05.17 1999.05.17 1999.05.17 1999.05.17;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.17D00:00:00.000000000 1999...\n1999.05.18| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.18 1999.05.18 1999.05.18 1999.05.18 1999.05.18 1999.05.18 1999.05.18;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.18D00:00:00.000000000 1999...\n1999.05.19| +`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit!(1999.05.19 1999.05.19 1999.05.19 1999.05.19 1999.05.19 1999.05.19 1999.05.19;`7203.T`AAPL`AMZN`MSFT`QQQ`SPY`VOW3.DE;1999.05.19D00:00:00.000000000 1999...\n"})}),"\n",(0,i.jsxs)(t.p,{children:["With this dictionary in hand, we can now leverage the ",(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/iterators#binary-opplication-of-each",children:"each-both"})," iterator to loop over our dataset and save the data for each day individually. Let\u2019s go ahead and do that."]}),"\n",(0,i.jsx)(t.admonition,{type:"tip",children:(0,i.jsxs)(t.p,{children:["You can find a detailed explanation of every iterator in my dedicated blog post: ",(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/iterators",children:"Iterators - Navigating Vectors Without the Need for Loops"}),"."]})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:'q)\\ts {[hdbDir;d;t] `stocks set t;.Q.dpft[hdbDir;d;`sym;`stocks];delete stocks from `.;}[`$":/Users/alexanderunterrainer/repos/financeData/hdb/"]\'[key g;value g:data group data`date]\n12766 13391008\nq)\\ts {[hdbDir;d;t] `stocks set t;.Q.dpft[hdbDir;d;`sym;`stocks];delete stocks from `.;}[`$":/Users/alexanderunterrainer/repos/financeData/hdb/"]\'[key g;value g:data group data`date]\n13449 13391008\n'})}),"\n",(0,i.jsxs)(t.p,{children:["Unfortunately, this approach didn\u2019t yield the performance gains I was hoping for, clocking in at 12.5 seconds, it's only marginally faster than our original method. Even after a second run (always a good idea when testing performance), the improvement was negligible. So it\u2019s clear we need a different strategy to optimise our save-down function. A quick win might be to leverage multithreading by using the ",(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/iterators#each-parallel",children:(0,i.jsx)(t.code,{children:"peach"})})," iterator to save each date in parallel. Let\u2019s give that a try."]}),"\n",(0,i.jsxs)(t.p,{children:["To take advantage of multithreading, we need to launch our KDB/Q process with multiple slave threads. This is done using the ",(0,i.jsx)(t.code,{children:"-s"})," command-line option, followed by the number of slaves you'd like to start. For our example, we\u2019ll go with eight slaves, a number chosen arbitrarily. After starting the process, we can confirm the number of active slave threads by running the system command ",(0,i.jsx)(t.code,{children:"\\s"})]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"alexanderunterrainer@Mac:~/repos/financeData/stock_data|\u21d2  qq ../draft.q -s 8\nKDB+ 4.1 2025.02.18 Copyright (C) 1993-2025 Kx Systems\nm64/ 8(24)core 24576MB alexanderunterrainer mac 192.168.1.177 EXPIRE 2026.03.11 KDB PLUS PERSONAL #5024911\n\nq)\\s\n8i\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Now, let\u2019s update our function to use ",(0,i.jsx)(t.code,{children:"peach"})," (parallel each) in place of the standard ",(0,i.jsx)(t.code,{children:"each"}),", and then run it to see the performance impact."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Modified saveData function:"})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:'saveDataP:{[] \n\td:exec distinct date from data;\n\tsaveOneDate[`$":/Users/alexanderunterrainer/repos/financeData/hdb/";data;] peach d;}\n'})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Test run:"})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:'q)\\ts saveDataP[]\n\'noupdate: `. `stocks\n  [4]  (.q.set)\n\n  [3]  /Users/alexanderunterrainer/repos/financeData/draft.q:15: saveOneDate:{[hdbDir;t;d]\n `stocks set select from t where date=d;\n         ^\n .Q.dpft[hdbDir;d;`sym;`stocks];\n  [1]  saveDataP:{[] d:exec distinct date from data;saveOneDate[`$":/Users/alexanderunterrainer/repos/financeData/hdb/";data;] peach d;}\n'})}),"\n",(0,i.jsxs)(t.p,{children:["To our surprise, this threw an error, but was it really unexpected? Not quite. We just ran into a well-known limitation of multi-threading and ",(0,i.jsx)(t.code,{children:"peach"}),": you can\u2019t modify a global variable, like ",(0,i.jsx)(t.code,{children:"stocks"})," in our case, within a parallel execution context. And that makes perfect sense. There\u2019s no way to ensure multiple threads aren\u2019t simultaneously reading from or, worse, writing to the same global variable. Thankfully, this is only a minor setback. All we need to do is refactor our ",(0,i.jsx)(t.code,{children:"saveOneDate"})," function to avoid relying on global variables. A simple fix."]}),"\n",(0,i.jsx)(t.admonition,{type:"note",children:(0,i.jsxs)(t.p,{children:["We'll also need to make a slight adjustment to our ",(0,i.jsx)(t.code,{children:"saveData"})," function to accommodate the changes."]})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"saveOneDateP:{[hdbDir;t;d] \n\ttoSave:delete date from update `p#sym from `sym`feedHandlerTime xasc select from t where date=d;\n\t(` sv (.Q.par[hdbDir;d;`stocks];`)) set toSave\n\t}\n\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Since we're no longer using the built-in ",(0,i.jsx)(t.code,{children:".Q.dpft"})," function, we need to handle a few steps manually. But as you'll see, this extra effort is worthwhile, it results in noticeable performance gains. In the first line of our updated function, we perform several steps in one go. Reading from right to left:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"We begin by selecting only the rows for the specific date we want to save."}),"\n",(0,i.jsxs)(t.li,{children:["We then sort the data first by ",(0,i.jsx)(t.code,{children:"sym"})," and, within each symbol, by ",(0,i.jsx)(t.code,{children:"feedHandlerTime"}),"."]}),"\n",(0,i.jsxs)(t.li,{children:["Next, we apply the ",(0,i.jsx)(t.code,{children:"parted"})," attribute to the ",(0,i.jsx)(t.code,{children:"sym"})," column, a step that ",(0,i.jsx)(t.code,{children:".Q.dpft"})," previously handled for us."]}),"\n",(0,i.jsxs)(t.li,{children:["Finally, we remove the ",(0,i.jsx)(t.code,{children:"date"})," column, since it will now be represented as a partition directory in our HDB."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["As the final step, we construct the file path for storing that day's data using ",(0,i.jsx)(t.code,{children:".Q.par"}),", combining the HDB root directory, table name, and target date. We then save the processed data directly to that path."]}),"\n",(0,i.jsxs)(t.p,{children:["Now let\u2019s take a look at the changes we need to apply to the ",(0,i.jsx)(t.code,{children:"saveData"})," function."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:'saveDataP:{[data] \n\tdata:.Q.en[hdbDir:`$":/Users/alexanderunterrainer/repos/financeData/hdb/"] data;\n\td:exec distinct date from data;\n\tsaveOneDateP[hdbDir;data;] peach d\n\t}\n'})}),"\n",(0,i.jsxs)(t.p,{children:["Another task we now need to handle manually now that we no longer use ",(0,i.jsx)(t.code,{children:".Q.dpft"})," is enumeration, a key concept we'll explore in more detail in a future blog post."]}),"\n",(0,i.jsxs)(t.p,{children:["To accommodate this, we modify the ",(0,i.jsx)(t.code,{children:"saveData"})," function so it accepts the dataset to be saved as a parameter. Within the function, we perform the following steps:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["First, we enumerate the data using the ",(0,i.jsx)(t.code,{children:"sym"})," file located in the root directory of our HDB."]}),"\n",(0,i.jsx)(t.li,{children:"Next, we retrieve all unique dates from the dataset"}),"\n",(0,i.jsxs)(t.li,{children:["And finally, we use ",(0,i.jsx)(t.code,{children:"peach"})," to invoke our optimized ",(0,i.jsx)(t.code,{children:"saveOneDateP"})," function for each distinct date in parallel."]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Next, let's put our newly optimized function to the test and compare its performance against the original implementation."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)\\ts saveDataP[data]\n6056 4195184\n"})}),"\n",(0,i.jsx)(t.p,{children:"The result speak for itself, we managed to cut the execution time by more than half. That\u2019s a significant improvement, especially considering how minimal the code changes were. Even with the enhancements, the total lines of code remain under twenty."}),"\n",(0,i.jsx)(t.p,{children:"But is this the best we can do? We owe it to ourselves to at least give it another shot."}),"\n",(0,i.jsx)(t.h4,{id:"behind-the-magic-the-secret-life-of-qpdft",children:"Behind the Magic: The Secret Life of .Q.pdft"}),"\n",(0,i.jsxs)(t.p,{children:["For our next round of performance tuning, we\u2019ll dive into the inner workings of ",(0,i.jsx)(t.code,{children:".Q.dpft"})," to see if we can repurpose some of its internal logic to our advantage. But before we do so, let's first look beyond the dot and understand how ",(0,i.jsx)(t.code,{children:".Q.dpft"})," works behind the curtains. As a reminder:"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:".Q.dpft[d; p; f; t]\n"})}),"\n",(0,i.jsx)(t.p,{children:"where:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"d"})," is the directory handle for the HDB"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"p"})," is the partition (typically a date)"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"f"})," is the column used for partitioning (must exist in the table since version 4.1t 2021.09.03)"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"t"})," is the name (as a symbol) of the simple table to be saved, where columns are vectors or compound lists."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["With that structure in mind, let\u2019s explore how ",(0,i.jsx)(t.code,{children:".Q.dpft"})," is implemented internally and see what components we can reuse or adapt for even better performance."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q).Q.dpft\nk){[d;p;f;t;s]\n      if[` in f,c:!+r:`. . `\\:t;'`domain];\n      if[~f in c;'f];\n      i:<t f;\n      r:+enxs[$;d;r;s];\n      {[d;t;i;u;x]@[d;x;:;u t[x]i]}[d:par[d;p;t];r;i;]'[(::;`p#)f=c;c];\n      @[d;`.d;:;f,c@&~f=c];t}[;;;;`sym]\n"})}),"\n",(0,i.jsxs)(t.p,{children:["As is often the case, ",(0,i.jsx)(t.code,{children:"K"})," code tends to be even more concise and cryptic than ",(0,i.jsx)(t.code,{children:"Q"}),", making it challenging to read, especially for the untrained eye. That said, the core functionality of the ",(0,i.jsx)(t.code,{children:".Q.dpft"})," function can be broken down into six key steps:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Store the table content"})," and ",(0,i.jsx)(t.strong,{children:"Extract column names"})," from table ",(0,i.jsx)(t.code,{children:"t"})," and validate that no unexpected columns are present."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Verify"})," that the column targeted for the parted attribute exists within the table."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Determine index order"})," using ",(0,i.jsx)(t.code,{children:"iasc"})," to sort the table appropriately."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Enumerate"})," the table against the ",(0,i.jsx)(t.code,{children:"sym"})," file to ensure consistency in symbol representation."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Persist data"}),", saving each column individually while reordering and applying attributes as needed."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsxs)(t.strong,{children:["Write the ",(0,i.jsx)(t.code,{children:".d"})," file"]}),", which contains the column names in their correct order"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Now, let\u2019s dive deeper into each of these steps to better understand how we might adopt and adapt this approach."}),"\n",(0,i.jsxs)(t.p,{children:["In the first line, we use an ",(0,i.jsx)(t.code,{children:"if"})," statement to ensure there are no unexpected columns in the table we want to persist. However, the condition inside that ",(0,i.jsx)(t.code,{children:"if"})," does quite a bit, so let\u2019s unpack it:"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"` in f,c:!+r:`. . `\\:t\n"})}),"\n",(0,i.jsxs)(t.p,{children:["As we are well aware by now, we read ",(0,i.jsx)(t.code,{children:"Q"})," and ",(0,i.jsx)(t.code,{children:"K"})," from right to left. Starting on the far right:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"`\\:t"})," is ",(0,i.jsx)(t.code,{children:"K"})," code for  the ",(0,i.jsxs)(t.a,{href:"https://code.kx.com/q/ref/vs/",children:[(0,i.jsx)(t.code,{children:"vs"})," (vector-from-scalar)"]})," operator to convert the table name ",(0,i.jsx)(t.code,{children:"t"}),", a symbol atom, into a single-element list"]}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)vs\nk){x\\:y}\nq)` vs `stock\n,`stock\nq)k)`\\:`stock\n,`stock\n"})}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"`. . `\\:t"})," performs a dot-apply to retrieve the actual table value from the general namespace, giving us the contents of the table name stored in ",(0,i.jsx)(t.code,{children:"t"}),". The result is assigned to variable ",(0,i.jsx)(t.code,{children:"r"})]}),"\n"]}),"\n",(0,i.jsx)(t.admonition,{type:"tip",children:(0,i.jsxs)(t.p,{children:["If you'd like a refresher or a deep dive into how the powerful apply operator works in KDB/Q, make sure to check out my dedicated blog post: ",(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/amend",children:"Amend, Amend At \u2013 The Swiss Army Knife of KDB/Q Operators"}),"."]})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"// Q code\nq)`. . ` vs `data\ndate       sym    feedHandlerTime               open     high     low      close    volume   dividends stockSplit\n-----------------------------------------------------------------------------------------------------------------\n1999.05.06 7203.T 1999.05.06D00:00:00.000000000 396.8345 410.4792 392.2863 410.4792 15575000 0         0\n1999.05.07 7203.T 1999.05.07D00:00:00.000000000 408.2051 409.3421 392.2862 395.6974 15165000 0         0\n1999.05.10 7203.T 1999.05.10D00:00:00.000000000 395.6975 400.2458 392.2863 394.5605 6305000  0         0\n1999.05.11 7203.T 1999.05.11D00:00:00.000000000 397.9717 400.2458 391.1493 391.1493 8430000  0         0\n1999.05.12 7203.T 1999.05.12D00:00:00.000000000 391.1492 397.9716 391.1492 397.9716 12980000 0         0\n1999.05.13 7203.T 1999.05.13D00:00:00.000000000 393.4233 393.4233 385.4639 388.8751 7165000  0         0\n1999.05.14 7203.T 1999.05.14D00:00:00.000000000 390.0121 390.0121 383.1897 384.3268 11475000 0         0\n...\n\n// K code\nq)k)`. . `\\:`data\ndate       sym    feedHandlerTime               open     high     low      close    volume   dividends stockSplit\n-----------------------------------------------------------------------------------------------------------------\n1999.05.06 7203.T 1999.05.06D00:00:00.000000000 396.8345 410.4792 392.2863 410.4792 15575000 0         0\n1999.05.07 7203.T 1999.05.07D00:00:00.000000000 408.2051 409.3421 392.2862 395.6974 15165000 0         0\n1999.05.10 7203.T 1999.05.10D00:00:00.000000000 395.6975 400.2458 392.2863 394.5605 6305000  0         0\n1999.05.11 7203.T 1999.05.11D00:00:00.000000000 397.9717 400.2458 391.1493 391.1493 8430000  0         0\n1999.05.12 7203.T 1999.05.12D00:00:00.000000000 391.1492 397.9716 391.1492 397.9716 12980000 0         0\n1999.05.13 7203.T 1999.05.13D00:00:00.000000000 393.4233 393.4233 385.4639 388.8751 7165000  0         0\n1999.05.14 7203.T 1999.05.14D00:00:00.000000000 390.0121 390.0121 383.1897 384.3268 11475000 0         0\n...\n"})}),"\n",(0,i.jsxs)(t.p,{children:["The remainder of the line is fairly straightforward. We use the ",(0,i.jsx)(t.code,{children:"+"})," operator in ",(0,i.jsx)(t.code,{children:"K"})," (which corresponds to the ",(0,i.jsx)(t.a,{href:"https://code.kx.com/q/ref/flip/",children:(0,i.jsx)(t.code,{children:"flip"})})," operator in ",(0,i.jsx)(t.code,{children:"Q"}),") to convert our table into a dictionary, and then apply the ",(0,i.jsx)(t.code,{children:"!"})," (",(0,i.jsx)(t.code,{children:"key"})," in ",(0,i.jsx)(t.code,{children:"Q"}),") operator to extract the column names, storing them in the variable ",(0,i.jsx)(t.code,{children:"c"}),". We then concatenate the column name of the column we want to apply the ",(0,i.jsx)(t.code,{children:"parted"})," attribute to with this list of column names. Finally, we check if the empty symbol ",(0,i.jsx)(t.code,{children:"`"})," appears in that list, if it does, we throw a ",(0,i.jsx)(t.code,{children:"domain"})," error; if not, we proceed with the function's execution. Not too bad, right? Let's back this up with some code examples."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)flip\n+:\nq)key\n!:\nq)flip data\ndate           | 1999.05.06                    1999.05.07                    1999.05.10                    1999.05.11                    1999.05.12                    1999.05.13                    1999.05.14                    1999.05.17  ..\nsym            | 7203.T                        7203.T                        7203.T                        7203.T                        7203.T                        7203.T                        7203.T                        7203.T      ..\nfeedHandlerTime| 1999.05.06D00:00:00.000000000 1999.05.07D00:00:00.000000000 1999.05.10D00:00:00.000000000 1999.05.11D00:00:00.000000000 1999.05.12D00:00:00.000000000 1999.05.13D00:00:00.000000000 1999.05.14D00:00:00.000000000 1999.05.17D0..\nopen           | 396.8345                      408.2051                      395.6975                      397.9717                      391.1492                      393.4233                      390.0121                      378.6415    ..\nhigh           | 410.4792                      409.3421                      400.2458                      400.2458                      397.9716                      393.4233                      390.0121                      379.7786    ..\nlow            | 392.2863                      392.2862                      392.2863                      391.1493                      391.1492                      385.4639                      383.1897                      371.8192    ..\nclose          | 410.4792                      395.6974                      394.5605                      391.1493                      397.9716                      388.8751                      384.3268                      372.9562    ..\nvolume         | 15575000                      15165000                      6305000                       8430000                       12980000                      7165000                       11475000                      10900000    ..\ndividends      | 0                             0                             0                             0                             0                             0                             0                             0           ..\nstockSplit     | 0                             0                             0                             0                             0                             0                             0                             0           ..\nq)key flip data\n`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit\nq)` in `sym,key flip data\n0b\nq)\n"})}),"\n",(0,i.jsxs)(t.p,{children:["That\u2019s a lot happening in just one line! Don\u2019t worry though, he next few lines are much simpler. In the following line, we just check whether the column we want to apply the ",(0,i.jsx)(t.code,{children:"parted"})," attribute to is actually present in the table\u2019s column names. If it\u2019s not (",(0,i.jsx)(t.code,{children:"~"})," is the ",(0,i.jsx)(t.code,{children:"K"})," operator for logical ",(0,i.jsx)(t.code,{children:"not"}),"), we raise an error and halt execution."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)not\n~:\nq)not `sym in `hello`world`test\n1b\nq)not `sym in `hello`world`test`sym\n0b\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Next, we retrieve the sorted (ascending) indices of the column to which we want to apply the ",(0,i.jsx)(t.code,{children:"parted"})," attribute. This allows us to reorder all other columns in the same sequence. We start by extracting the target column from the table using its name, then apply the ",(0,i.jsx)(t.code,{children:"<"})," operator, the ",(0,i.jsx)(t.code,{children:"K"})," equivalent of ",(0,i.jsx)(t.code,{children:"Q's"})," ",(0,i.jsx)(t.code,{children:"iasc"})," (You can disregard the initial part of the underlying ",(0,i.jsx)(t.code,{children:"iasc"})," implementation, it simply checks whether the input is a list rather than a single atomic value.) This operator returns the indices that would sort the list in ascending order. When you use these indices to reindex the original list, the result is a sorted version of that list."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)data`sym\n`s#`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203.T`7203..\nq)iasc\nk){$[0h>@x;'`rank;<x]}\nq)iasc 3 4 9 1 2 3 6 7 1 2\n3 8 4 9 0 5 1 6 7 2\nq)l iasc l:3 4 9 1 2 3 6 7 1 2\n1 1 2 2 3 3 4 6 7 9\n"})}),"\n",(0,i.jsxs)(t.p,{children:["The next line might look a bit cryptic at first glance, but all it's doing is enumerating the table against the sym file. How do I know? By examining the underlying ",(0,i.jsx)(t.code,{children:"K"})," code of the ",(0,i.jsx)(t.a,{href:"https://code.kx.com/q/ref/dotq/#en-enumerate-varchar-cols",children:(0,i.jsx)(t.code,{children:".Q.en"})})," function, which is used to enumerate tables, it's clear that it closely mirrors the implementation of ",(0,i.jsx)(t.code,{children:".Q.enxs"}),". In fact, ",(0,i.jsx)(t.code,{children:".Q.en"})," is simply a projection of ",(0,i.jsx)(t.code,{children:".Q.enxs"})," with two parameters pre-applied."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q).Q.en\nk){[x;d;t;s]if[(~(::)~d)&#c:&{$[11h=@*x;&/11h=@:'x;11h=@x]}'+t;(`/:d,s)??,/(?,/)'t c];@[t;c;{$[@y;x y;(-1_+\\0,#:'y)_x@,/y]}x s]}[;;;`sym][?]\nq).Q.enxs\nk){[x;d;t;s]if[(~(::)~d)&#c:&{$[11h=@*x;&/11h=@:'x;11h=@x]}'+t;(`/:d,s)??,/(?,/)'t c];@[t;c;{$[@y;x y;(-1_+\\0,#:'y)_x@,/y]}x s]}\nq)\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Now comes the line where most of the magic happens. At a high level, the lambda function here takes care of sorting each column based on the ascending indexes we previously calculated, applies the ",(0,i.jsx)(t.code,{children:"parted"})," attribute when required, and then saves each column to disk. But let\u2019s peel back the layers and break it down step by step to make it more approachable."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"{[d;t;i;u;x] @[d;x;:;u t[x]i] }[d:par[d;p;t];r;i;]'[(::;`p#)f=c;c]\n"})}),"\n",(0,i.jsxs)(t.p,{children:["We can observe that the lambda function takes five parameters, with three of them already projected onto the function. This implies that the ",(0,i.jsx)(t.code,{children:"'"})," operator used afterward is the ",(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/iterators#binary-opplication-of-each",children:(0,i.jsx)(t.em,{children:(0,i.jsx)(t.strong,{children:"each-both"})})})," iterator, meaning the lambda is being applied across the remaining two parameters simultaneously, those defined in the final part of the line."]}),"\n",(0,i.jsxs)(t.p,{children:["Recall that ",(0,i.jsx)(t.code,{children:"c"})," holds all column names and ",(0,i.jsx)(t.code,{children:"f"})," contains the name of the column we want to apply the ",(0,i.jsx)(t.code,{children:"parted"})," attribute to. The second parameter of ",(0,i.jsx)(t.code,{children:"[(::;p#)f=c;c]"})," is straightforward, it\u2019s simply the list of column names."]}),"\n",(0,i.jsxs)(t.p,{children:["The first parameter becomes clear when broken down: ",(0,i.jsx)(t.code,{children:"f=c"})," compares the column ",(0,i.jsx)(t.code,{children:"f"})," to each element in ",(0,i.jsx)(t.code,{children:"c"})," (this works thanks to ",(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/iterators#implicit-iteration",children:"implicit iteration"}),"), producing a boolean list. This boolean mask is then used to index into the list on the left, which consists of two functions, the identity function ",(0,i.jsx)(t.code,{children:"::"})," and a function which applies the parted attribute ",(0,i.jsx)(t.code,{children:"`p#"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["The result of this indexing is a list of functions, for each column, we either apply the identity (if it's not ",(0,i.jsx)(t.code,{children:"f"}),") or apply the parted attribute (if it is ",(0,i.jsx)(t.code,{children:"f"}),"). These functions can then be mapped to the corresponding columns, as demonstrated in the code that follows."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)show c:`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit\n`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit\nq)show f:`sym\n`sym\nq)f=c\n0100000000b\nq)(::;`p#)f=c\n::\n#[`p]\n::\n::\n::\n::\n::\n::\n::\n::\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Next, let\u2019s examine the parameters that are projected onto our lambda function. The variables ",(0,i.jsx)(t.code,{children:"r"})," and ",(0,i.jsx)(t.code,{children:"i"})," are fairly straightforward: ",(0,i.jsx)(t.code,{children:"i"})," holds the list of indices that would sort the ",(0,i.jsx)(t.code,{children:"sym"})," column in ascending order, these indices are then used to reorder all other columns accordingly. ",(0,i.jsx)(t.code,{children:"r"})," contains the enumerated version of our dataset, which we generated earlier."]}),"\n",(0,i.jsxs)(t.p,{children:["Now let's take a closer look at the first parameter, ",(0,i.jsx)(t.code,{children:"d:par[d;p;t]"}),". Here, ",(0,i.jsx)(t.code,{children:"d"})," is the root directory of our Historical Database, ",(0,i.jsx)(t.code,{children:"p"})," is the partition date we\u2019re currently working with, and ",(0,i.jsx)(t.code,{children:"t"})," is the table name, passed in as a symbol. The ",(0,i.jsx)(t.a,{href:"https://code.kx.com/q/ref/dotq/#par-get-expected-partition-location",children:(0,i.jsx)(t.code,{children:".Q.par"})})," function takes these three arguments and constructs a full path by joining them. The resulting connection string is then reassigned to ",(0,i.jsx)(t.code,{children:"d"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["Now that we've covered all the parameters, both those projected onto the lambda and those iterated over using ",(0,i.jsx)(t.code,{children:"each-both"}),", let\u2019s look at what actually happens inside the lambda itself. Fortunately, it\u2019s quite straightforward. The lambda uses ",(0,i.jsx)(t.code,{children:"apply-at (@)"})," to store each column in the correct location on disk. Here's how it works: we first extract the relevant column from the table using its name ",(0,i.jsx)(t.code,{children:"t[x]"}),", then reorder its values using the indices stored in ",(0,i.jsx)(t.code,{children:"i"})," to ensure they align with the sorted ",(0,i.jsx)(t.code,{children:"sym"})," column. Next, we apply the appropriate transformation from ",(0,i.jsx)(t.code,{children:"u"}),": either the identity function ",(0,i.jsx)(t.code,{children:"::"})," (which leaves the column unchanged) or the ",(0,i.jsx)(t.code,{children:"`p#"})," function (which applies the parted attribute). Finally, we use ",(0,i.jsx)(t.code,{children:"apply-at"})," to write the processed column to disk."]}),"\n",(0,i.jsx)(t.p,{children:"Not too bad after all, right?"}),"\n",(0,i.jsxs)(t.p,{children:["The final line of the ",(0,i.jsx)(t.code,{children:".Q.pdft"})," function is simple but essential, it creates the ",(0,i.jsx)(t.code,{children:".d"})," file for the current partition. This file lists the column names, with the column to which we applied the parted attribute appearing first. This is achieved with the expression ",(0,i.jsx)(t.code,{children:"f,c@&~f=c"}),". Here's what it does:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"f=c"})," compares the parted column name (",(0,i.jsx)(t.code,{children:"f"}),") with all column names in ",(0,i.jsx)(t.code,{children:"c"}),", returning a boolean mask."]}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)f\n`sym\nq)c\n`date`sym`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit\nq)f=c\n0100000000b\n"})}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"~"})," (",(0,i.jsx)(t.code,{children:"not"}),") inverts the mask, identifying columns ",(0,i.jsx)(t.em,{children:(0,i.jsx)(t.strong,{children:"other than"})})," ",(0,i.jsx)(t.code,{children:"f"})]}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)not\n~:\nq)k)~f=c\n1011111111b\nq)not f=c\n1011111111b\n"})}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"&"})," (",(0,i.jsx)(t.code,{children:"where"}),") returns the indices of those non-matching columns."]}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)where\n&:\nq)where not f=c\n0 2 3 4 5 6 7 8 9\nq)k)&~f=c\n0 2 3 4 5 6 7 8 9\n"})}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.code,{children:"c@&~f=c"})," indexes into the list of column names to retrieve only the non-parted columns."]}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)c@where not f=c\n`date`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit\nq)k)c@&~f=c\n`date`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit\n"})}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["These are then appended to ",(0,i.jsx)(t.code,{children:"f"}),", putting the parted column first."]}),"\n"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"q)f,c@where not f=c\n`sym`date`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit\nq)k)f,c@&~f=c\n`sym`date`feedHandlerTime`open`high`low`close`volume`dividends`stockSplit\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Finally, we use another ",(0,i.jsx)(t.code,{children:"amend-at"})," operation to write this ordered list of column names into the ",(0,i.jsx)(t.code,{children:".d"})," file on disk."]}),"\n",(0,i.jsx)(t.h4,{id:"performance-tuning-part-2",children:"Performance Tuning Part 2"}),"\n",(0,i.jsxs)(t.p,{children:["Now that we understand the inner workings of ",(0,i.jsx)(t.code,{children:".Q.pdft"}),", we can borrow some of its techniques to build a more efficient save-down function of our own."]}),"\n",(0,i.jsxs)(t.p,{children:["The idea is to take our parallelism one step further, not only will we save each day in parallel, but we\u2019ll also save each ",(0,i.jsx)(t.em,{children:(0,i.jsx)(t.strong,{children:"column"})})," in parallel. This is something ",(0,i.jsx)(t.code,{children:".Q.dpft"})," doesn\u2019t do. Since KDB+/Q is a column-oriented database where each column is stored as a separate vector (and thus a separate file), we can safely write columns concurrently without risking file access conflicts."]}),"\n",(0,i.jsx)(t.p,{children:"We\u2019ll implement two variations of this approach:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Column-level sorting"}),":each"," column is sorted individually right before it's written to disk."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Table-level sorting"}),": we sort the entire table once and then save each column independently."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["This allows us to compare whether repeatedly sorting individual columns is faster than a single full-table sort followed by column saves. You can find the complete source code in my ",(0,i.jsx)(t.a,{href:"https://github.com/DefconQ/defconQ/tree/master/projects/researchHDB",children:"repository"}),". For now, let\u2019s focus on the performance comparison."]}),"\n",(0,i.jsx)(t.p,{children:"Once again, we launch our KDB+/Q process with 8 slave threads and benchmark the performance of our newly optimized functions."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"alexanderunterrainer@Mac:~/repos/financeData/stock_data|\u21d2  qq ../researchHDB.q -s 8\nKDB+ 4.1 2025.02.18 Copyright (C) 1993-2025 Kx Systems\nm64/ 8(24)core 24576MB alexanderunterrainer mac 192.168.1.177 EXPIRE 2026.03.11 KDB PLUS PERSONAL #5024911\n\nq)loadData each files\nq)count data\n75824\nq)\\ts saveDataP1[data]\n6647 4196640\nq)\\ts saveDataP2[data]\n7126 14154496\n"})}),"\n",(0,i.jsx)(t.p,{children:"As you can see, our performance tuning unfortunately didn\u2019t yield the improvements we had hoped for. This could be due to a couple of reasons:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"The table for each date is relatively small, with only 12 different stocks, the overall volume of data might not be large enough to benefit from further parallelization."}),"\n",(0,i.jsx)(t.li,{children:"Additionally, the table isn\u2019t very wide, containing only a handful of columns. This limits the gains we could achieve from parallel column-wise writes."}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["That said, this section served as a valuable exercise in understanding the inner workings of the ",(0,i.jsx)(t.code,{children:".Q.dpft"})," function and exploring some advanced KDB+/Q techniques."]}),"\n",(0,i.jsx)(t.h4,{id:"historical-research-database-hdb",children:"Historical Research Database (HDB)"}),"\n",(0,i.jsx)(t.p,{children:"The Historical Database (HDB) we set up for this project is as straightforward as it gets. We simply point our KDB+/Q process to the root directory of the HDB and load that path into memory. This action memory-maps all the data in the directory, making it immediately available for querying."}),"\n",(0,i.jsx)(t.h2,{id:"putting-it-all-together",children:"Putting it all together"}),"\n",(0,i.jsx)(t.p,{children:"In this section, I\u2019ll walk you through how to use the code we\u2019ve covered to build your own research HDB."}),"\n",(0,i.jsx)(t.p,{children:"Start by downloading historical stock data from Yahoo Finance using the provided Python data downloader. Begin executing the script with:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"python3 stock_data_downloader.py --period max\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Once the download is complete, confirm the data is there by running the ",(0,i.jsx)(t.code,{children:"ls"}),"command in your terminal: you should see a ",(0,i.jsx)(t.code,{children:"stock_data"})," directory containing CSV files with the stock data."]}),"\n",(0,i.jsxs)(t.p,{children:["Next, ",(0,i.jsx)(t.code,{children:"cd"})," into the ",(0,i.jsx)(t.code,{children:"stock_data"})," directory and start a KDB+/Q process."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"alexanderunterrainer@Mac:~/repos/defconQ/projects/researchHDB/stock_data|master\u26a1 \u21d2  qq ../researchHdb.q -s 8\nKDB+ 4.1 2025.02.18 Copyright (C) 1993-2025 Kx Systems\nm64/ 8(24)core 24576MB alexanderunterrainer mac 192.168.1.177 EXPIRE 2026.03.11 KDB PLUS PERSONAL #5024911\n\nq)loadData each files\nq)count data\n75863\nq)saveDataP1[data]\n\n"})}),"\n",(0,i.jsxs)(t.p,{children:["Load the corresponding ",(0,i.jsx)(t.code,{children:".q"})," file, then run the ",(0,i.jsx)(t.code,{children:"loadData"})," function to load the stock data into memory. After that, use the ",(0,i.jsx)(t.code,{children:"saveData"})," function to write the data to disk (you can choose whichever version of ",(0,i.jsx)(t.code,{children:"saveData"})," you prefer)."]}),"\n",(0,i.jsx)(t.p,{children:"Once the data has been persisted, you\u2019re ready to start your research HDB process, and that\u2019s it! You\u2019re now all set to begin working with your historical stock data."}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{children:"alexanderunterrainer@Mac:~/repos/financeData|\u21d2  qq hdb\nKDB+ 4.1 2025.02.18 Copyright (C) 1993-2025 Kx Systems\nm64/ 8(24)core 24576MB alexanderunterrainer mac 192.168.1.177 EXPIRE 2026.03.11 KDB PLUS PERSONAL #5024911\n\nq)count stocks\n75863\nq)meta stocks\nc              | t f a\n---------------| -----\ndate           | d\nsym            | s   p\nfeedHandlerTime| p\nopen           | f\nhigh           | f\nlow            | f\nclose          | f\nvolume         | j\ndividends      | f\nstockSplit     | f\n"})}),"\n",(0,i.jsx)(t.h2,{id:"where-we-go-from-here",children:"Where we go from here"}),"\n",(0,i.jsx)(t.p,{children:"This project offers a simple yet powerful way to download historical stock market data for daily end-of-day price, completely free of charge. As you've seen, the code is fairly straightforward. From here, you might consider enhancing the data loading process or developing a research tool around your historical stock market database. You could even go a step further by integrating this with a real-time market data feed, building a query-routing gateway, and gradually creating your own full-featured framework. Whatever direction you choose, the key is to enjoy the process."}),"\n",(0,i.jsx)(t.p,{children:"Until next time\u2014happy coding!"}),"\n",(0,i.jsx)(t.h2,{id:"resources",children:"Resources"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["yfinance library: ",(0,i.jsx)(t.a,{href:"https://github.com/ranaroussi/yfinance",children:"Githup repo"})]}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/architecture/plain",children:"The Plain Vanilla Tick Setup"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/tutorials/tick",children:"KDB Tick Explained: A Walkthrough"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/attributes",children:"Attributes"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://www.defconq.tech/docs/concepts/dictionariesTables",children:"Dictionaries and Tables"})}),"\n"]}),"\n",(0,i.jsx)(t.h3,{id:"source-code",children:"Source code"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.a,{href:"https://github.com/DefconQ/defconQ/tree/master/projects",children:"DefconQ Github Project Repo"})})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},26662:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/backToTheFuture-5c80c6922bdea78e4dcfde7553ef1d6f.jpg"},53319:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/researchHDB-8f86cfa2c76c567983e91ee9148603cf.png"},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>r});var i=n(96540);const s={},o=i.createContext(s);function a(e){const t=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:t},e.children)}}}]);